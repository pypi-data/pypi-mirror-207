Metadata-Version: 2.1
Name: flatnet
Version: 0.2.0
Summary: FlatNet implementation in PyTorch, from the paper "Representation Learning via Manifold Flattening and Reconstruction"
Home-page: UNKNOWN
Author: Michael Psenka
Author-email: psenka@eecs.berkeley.edu
Maintainer: ['Druv Pai <druvpai@berkeley.edu>', 'Vishal Raman <vraman@berkeley.edu>']
License: UNKNOWN
Platform: UNKNOWN
Description-Content-Type: text/markdown
Requires-Dist: Jinja2 (>=3.1.2)
Requires-Dist: MarkupSafe (>=2.1.2)
Requires-Dist: cmake (>=3.26.3)
Requires-Dist: filelock (>=3.12.0)
Requires-Dist: geoopt (>=0.5.0)
Requires-Dist: imageio (>=2.28.1)
Requires-Dist: lit (>=16.0.2)
Requires-Dist: mpmath (>=1.3.0)
Requires-Dist: networkx (>=3.1)
Requires-Dist: numpy (>=1.24.3)
Requires-Dist: nvidia-cublas-cu11 (>=11.10.3.66)
Requires-Dist: nvidia-cuda-cupti-cu11 (>=11.7.101)
Requires-Dist: nvidia-cuda-nvrtc-cu11 (>=11.7.99)
Requires-Dist: nvidia-cuda-runtime-cu11 (>=11.7.99)
Requires-Dist: nvidia-cudnn-cu11 (>=8.5.0.96)
Requires-Dist: nvidia-cufft-cu11 (>=10.9.0.58)
Requires-Dist: nvidia-curand-cu11 (>=10.2.10.91)
Requires-Dist: nvidia-cusolver-cu11 (>=11.4.0.1)
Requires-Dist: nvidia-cusparse-cu11 (>=11.7.4.91)
Requires-Dist: nvidia-nccl-cu11 (>=2.14.3)
Requires-Dist: nvidia-nvtx-cu11 (>=11.7.91)
Requires-Dist: scipy (>=1.10.1)
Requires-Dist: sympy (>=1.11.1)
Requires-Dist: torch (>=2.0.0)
Requires-Dist: tqdm (>=4.65.0)
Requires-Dist: triton (>=2.0.0)
Requires-Dist: typing-extensions (>=4.5.0)

This is a minimal pip package to allow easy deployment of FlatNets, a geometry-based neural autoencoder architecture that automatically builds its layers based on geometric properties of the dataset. See our paper, [Representation Learning via Manifold Flattening and Reconstruction](https://arxiv.org/abs/2305.01777), for more details, and [the Github repo](https://github.com/michael-psenka/manifold-linearization) for the code and example scripts & notebooks.

Changelog
=========
0.2.0 (2023-05-07)
------------------
Features:
- Added easy gif saving! If your data is 2D, you can now visualize how your data evolves with the manifold flow.

0.1.4 (2023-05-06)
------------------
Bug Fixes:
- Remembered why I did not add normalization into main training file, now removed

0.1.3 (2023-05-06)
------------------

Bug Fixes:
- Added normalization of the input data

0.1.1 (2023-05-06)
------------------

Features:
- Added normalization of the input data

0.1.0 (2022-12-31)
------------------

Initial release of flatnet!

# Manifold Linearization for Representation Learning

This is a research project focused on the automatic generation of autoencoders with minimal feature size, when the data is supported near an embedded submanifold. Using the geometric structure of the manifold, we can equivalently treat this problem as a manifold flattening problem when the manifold is flattenable[^1]. See our paper, [Representation Learning via Manifold Flattening and Reconstruction](https://arxiv.org/abs/2305.01777), for more details.

[^1]: Geometric note: while flattenability is not general, there are some heuristic reasons we can motivate this assumption for real world data. For example, if a dataset permits a VAE-like autoencoder, where samples from the data distribution can be generated via a standard Gaussian in the latent space, then the samples lie close in probability to a flattenable manifold, as this VAE has constructed a single-chart atlas.

## Installation

The pure FlatNet construction (training) code is available as a pip package and can be installed in the following way:

```
pip install flatnet
```

This repo also contains a number of testing and illustrative files to both familiarize new users with the framework and show the experiments run in the main paper. To install the appropriate remaining dependencies for this repo, first navigate to the project directory, then run the following command:

```
pip install -r requirements.txt
```

## Quickstart usage

The follolwing is a simple example script using the pip package:

```python
import torch
import flatnet
import matplotlib.pyplot as plt

# create sine wave dataset
t = torch.linspace(0, 1, 50)
y = torch.sin(t * 2 * 3.14)

# format dataset of N points of dimension D as (N, D) matrix
X = torch.stack([t, y], dim=1)

# normalize data
X = (X - X.mean(dim=0)) / X.std(dim=0)

# f and g are both functions from R^D to R^D
f, g = flatnet.train(X, n_iter=50)

Z = f(X).detach().numpy()

plt.scatter(Z[:,0], Z[:,1])
plt.show()
```

The script `flatnet.py` includes many example experiments to run FlatNet constructions on. To see an example experiment, simply run `python flatnet_test.py` in the main directory to see the flattening and reconstruction of a simple sine wave. Further experiments and options can be specified through command line arguments, managed through [tyro](https://github.com/brentyi/tyro); to see the full list of arguments, run `python flatnet_test.py --help`.


## Directory Structure

- `flatnet_test.py`: main test script, as described in above section.
- `flatnet/train.py`: contains the main FlatNet construction (training) code.
- `flatnet/modules`: contains code for the neural network modules used in FlatNet.
- `experiments-paper`: contains scripts and results from experiments done in the paper.
- `models`: contains code for various models that FlatNet was compared against in the paper.
- `tools`: contains auxillery tools for evaulating the method, such as random manifold generators.


## Citation

If you use this work in your research, please cite the following paper:

```
@article{psenka2023flatnet,
  author = {Psenka, Michael and Pai, Druv and Raman, Vishal and Sastry, Shankar and Ma, Yi},
  title = {Representation Learning via Manifold Flattening and Reconstruction},
  year = {2023},
  eprint = {2305.01777},
  url = {https://arxiv.org/abs/2305.01777},
}
```

We hope that you find this project useful. If you have any questions or suggestions, please feel free to contact us.


