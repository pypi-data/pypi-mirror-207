{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"pytorch Hyperparameter Tuning with SPOT: Comparison with ray tuner on cifar10\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 60\n",
    "INIT_SIZE = 20\n",
    "DEVICE = \"cpu\" # \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12-torch_p040025_60min_20init_2023-05-07_21-33-26'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "start_time = datetime.now(tzlocal())\n",
    "HOSTNAME = socket.gethostname().split(\".\")[0]\n",
    "experiment_name = '12-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\n",
    "experiment_name = experiment_name.replace(':', '-')\n",
    "experiment_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Sequential Parameter Optimization Compared to Ray Tune\n",
    "## Hyperparameter Tuning: pytorch wth cifar10 Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook exemplifies hyperparameter tuning with SPOT (spotPython).\n",
    "* The hyperparameter software SPOT was developed in R (statistical programming language), see Open Access book \"Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide\", available here: [https://link.springer.com/book/10.1007/978-981-19-5170-1](https://link.springer.com/book/10.1007/978-981-19-5170-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotPython                                0.0.62\n",
      "spotRiver                                 0.0.92\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep  \"spot[RiverPython]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall spotPython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import copy\n",
    "import warnings\n",
    "import numbers\n",
    "import json\n",
    "import calendar\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from math import inf\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from spotPython.spot import spot\n",
    "from spotPython.hyperparameters.values import (\n",
    "    add_core_model_to_fun_control,\n",
    "    assign_values,\n",
    "    convert_keys,\n",
    "    get_bound_values,\n",
    "    get_default_hyperparameters_for_core_model,\n",
    "    get_default_values,\n",
    "    get_dict_with_levels_and_types,\n",
    "    get_values_from_dict,\n",
    "    get_var_name,\n",
    "    get_var_type,\n",
    "    iterate_dict_values,\n",
    "    modify_hyper_parameter_levels,\n",
    "    modify_hyper_parameter_bounds,\n",
    "    replace_levels_with_positions,\n",
    "    return_conf_list_from_var_dict,\n",
    "    get_one_core_model_from_X,\n",
    "    transform_hyper_parameter_values,\n",
    "    get_dict_with_levels_and_types,\n",
    "    convert_keys,\n",
    "    iterate_dict_values,\n",
    ")\n",
    "\n",
    "from spotPython.utils.convert import class_for_name\n",
    "from spotPython.utils.eda import (\n",
    "    get_stars,\n",
    "    gen_design_table)\n",
    "from spotPython.utils.transform import transform_hyper_parameter_values\n",
    "\n",
    "from spotPython.utils.convert import get_Xy_from_df\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.plot.validation import plot_cv_predictions, plot_roc, plot_confusion_matrix\n",
    "\n",
    "from spotPython.data.torch_hyper_dict import TorchHyperDict\n",
    "from spotPython.fun.hypertorch import HyperTorch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from spotPython.torch.netcifar10 import Net_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "MPS device:  mps\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS device: \", mps_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization of the Empty `fun_control` Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CIFAR Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loading is implemented as in the Section \"Data loaders\" in the PyTorch tutorial [Hyperparameter Tuning  with Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = load_data()\n",
    "train.data.shape, test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(train)\n",
    "# add the dataset to the fun_control\n",
    "fun_control.update({\"data\": None, # dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"target_column\": None})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specification of the Preprocessing Model\n",
    "\n",
    "Because the Ray Tune hyperparameter tuning does not use a preprocessing model, the preprocessing model is set to `None` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = []\n",
    "# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "# prep_model = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"categorical\", one_hot_encoder, categorical_columns),\n",
    "#         ],\n",
    "#         remainder=StandardScaler(),\n",
    "#     )\n",
    "prep_model = None\n",
    "fun_control.update({\"prep_model\": prep_model})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select `algorithm` and `core_model_hyper_dict`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model as implemented as in the Section \"Configurable neural network\" in the PyTorch tutorial [Hyperparameter Tuning  with Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) is used here. `spotPython` implements a class which is similar to the class described in the PyTorch tutorial. The class is called `Net_CIFAR10`   and is implemented in the file `netcifar10.py`. The class is imported here.\n",
    "\n",
    "Note: In addition to the class Net from the PyTorch tutorial, the class Net_CIFAR10 has additional attributes, namely:\n",
    "\n",
    "* learning rate (`lr`),\n",
    "* batchsize (`batch_size`),\n",
    "* epochs (`epochs`), and\n",
    "* k_folds (`k_folds`).\n",
    "\n",
    "Further attributes can be easily added to the class, e.g., `optimizer` or `loss_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_model = Net_CIFAR10\n",
    "fun_control = add_core_model_to_fun_control(core_model=core_model,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=TorchHyperDict,\n",
    "                              filename=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `hyper_dict` Hyperparameters for the Selected Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spotPython` uses simple `JSON` files for the specification of the hyperparameters. The `JSON` file for the `core_model` is called `torch_hyper_dict.json`. The corresponding entries for the `Net_CIFAR10` class are shown below.\n",
    "\n",
    "```json\n",
    "{\"Net_CIFAR10\":\n",
    "    {\n",
    "        \"l1\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 5,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 9},\n",
    "        \"l2\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 5,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 9},\n",
    "        \"lr\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 1e-03,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 1e-05,\n",
    "            \"upper\": 1e-02},\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 1,\n",
    "            \"upper\": 4},\n",
    "        \"epochs\": {\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 3,\n",
    "                \"transform\": \"transform_power_2_int\",\n",
    "                \"lower\": 1,\n",
    "                \"upper\": 4},\n",
    "        \"k_folds\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 2,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 3}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Each entry in the `JSON` file represents one hyperparameter with the following structure:\n",
    "`type`, `default`, `transform`, `lower`, and `upper`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In contrast to Ray Tune, `spotPython` can handle numerical, boolean, and categorical hyperparameters. Since `Ray Tune` does not tune categorical hyperparameters, they are not used here. However, they can be specified in the `JSON` file in a similar way as the numerical hyperparameters as shown below:\n",
    "\n",
    "```json\n",
    "\"factor_hyperparameter\": {\n",
    "    \"levels\": [\"A\", \"B\", \"C\"],\n",
    "    \"type\": \"factor\",\n",
    "    \"default\": \"B\",\n",
    "    \"transform\": \"None\",\n",
    "    \"core_model_parameter_type\": \"str\",\n",
    "    \"lower\": 0,\n",
    "    \"upper\": 2},\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modify `hyper_dict` Hyperparameters for the Selected Algorithm aka `core_model`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying the model, the corresponding hyperparameters, their types and bounds are loaded from the `JSON` file `torch_hyper_dict.json`. After loading, the user can modify the hyperparameters, e.g., the bounds.\n",
    "`spotPython` provides a clever rule for de-activating hyperparameters. If the lower and the upper bound are set to identical values, the hyperparameter is de-activated. This is useful for the hyperparameter tuning, because it allows to specify a hyperparameter in the `JSON` file, but to de-activate it in the `fun_control` dictionary. This is done in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify hyperparameter of type numeric and integer (boolean)\n",
    "\n",
    "Since the hyperparameter `k_folds` is not used in the PyTorch tutorial, it is de-activated here by setting the lower and upper bound to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': {'type': 'int',\n",
       "  'default': 5,\n",
       "  'transform': 'transform_power_2_int',\n",
       "  'lower': 2,\n",
       "  'upper': 9},\n",
       " 'l2': {'type': 'int',\n",
       "  'default': 5,\n",
       "  'transform': 'transform_power_2_int',\n",
       "  'lower': 2,\n",
       "  'upper': 9},\n",
       " 'lr': {'type': 'float',\n",
       "  'default': 0.001,\n",
       "  'transform': 'None',\n",
       "  'lower': 1e-05,\n",
       "  'upper': 0.01},\n",
       " 'batch_size': {'type': 'int',\n",
       "  'default': 4,\n",
       "  'transform': 'transform_power_2_int',\n",
       "  'lower': 1,\n",
       "  'upper': 4},\n",
       " 'epochs': {'type': 'int',\n",
       "  'default': 3,\n",
       "  'transform': 'transform_power_2_int',\n",
       "  'lower': 3,\n",
       "  'upper': 4},\n",
       " 'k_folds': {'type': 'int',\n",
       "  'default': 2,\n",
       "  'transform': 'None',\n",
       "  'lower': 0,\n",
       "  'upper': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_control = modify_hyper_parameter_bounds(fun_control, \"k_folds\", bounds=[0, 0])\n",
    "fun_control[\"core_model_hyper_dict\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify hyperparameter of type factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as for the numerical hyperparameters, the categorical hyperparameters can be modified. For example, the hyperparameter `leaf_model` is de-activated here by choosing only one value `\"LinearRegression\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_model\", [\"LinearRegression\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selection of the Objective (Loss) Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two metrics:\n",
    "\n",
    "    1. `metric` is used for the river based evaluation via `eval_oml_iter_progressive`.\n",
    "    2. `metric_sklearn` is used for the sklearn based evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**:\n",
    "\n",
    "* `spotPython` performs minimization by default.\n",
    "* If accuracy should be maximized, then the objective function has to be multiplied by -1. Therefore, `weights` is set to -1 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = HyperTorch(seed=123, log_level=50).fun_torch\n",
    "weights = 1.0\n",
    "shuffle = True\n",
    "eval = \"train_hold_out\"\n",
    "device = DEVICE\n",
    "\n",
    "fun_control.update({\n",
    "               \"data_dir\": None,\n",
    "               \"checkpoint_dir\": None,\n",
    "               \"horizon\": None,\n",
    "               \"oml_grace_period\": None,\n",
    "               \"weights\": weights,\n",
    "               \"step\": None,\n",
    "               \"log_level\": 50,\n",
    "               \"weight_coeff\": None,\n",
    "               \"metric\": None,\n",
    "               \"metric_sklearn\": None,\n",
    "               \"shuffle\": shuffle,\n",
    "               \"eval\": eval,\n",
    "               \"device\": device,\n",
    "               })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calling the SPOT Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the SPOT Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get types and variable names as well as lower and upper bounds for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "fun_control.update({\"var_type\": var_type,\n",
    "                    \"var_name\": var_name})\n",
    "\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name       | type   |   default |   lower |   upper | transform             |\n",
      "|------------|--------|-----------|---------|---------|-----------------------|\n",
      "| l1         | int    |     5     |   2     |    9    | transform_power_2_int |\n",
      "| l2         | int    |     5     |   2     |    9    | transform_power_2_int |\n",
      "| lr         | float  |     0.001 |   1e-05 |    0.01 | None                  |\n",
      "| batch_size | int    |     4     |   1     |    4    | transform_power_2_int |\n",
      "| epochs     | int    |     3     |   3     |    4    | transform_power_2_int |\n",
      "| k_folds    | int    |     2     |   0     |    0    | None                  |\n"
     ]
    }
   ],
   "source": [
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the `Spot` Optimizer\n",
    "\n",
    "* Run SPOT for approx. x mins (`max_time`).\n",
    "* Note: the run takes longer, because the evaluation time of initial design (here: `initi_size`, 20 points) is not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.e+00, 5.e+00, 1.e-03, 4.e+00, 3.e+00, 2.e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "hyper_dict=TorchHyperDict().load()\n",
    "X_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n",
    "X_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.309\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.462\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3073117743968963\n",
      "Accuracy on hold-out set: 0.10235\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.307\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.154\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.462\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.329\n",
      "Loss on hold-out set: 2.3057543350219727\n",
      "Accuracy on hold-out set: 0.10235\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.307\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3053957647800445\n",
      "Accuracy on hold-out set: 0.10235\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.306\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.154\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.576\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3093016027927398\n",
      "Accuracy on hold-out set: 0.09795\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.307\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.154\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.304451789188385\n",
      "Accuracy on hold-out set: 0.099\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.307\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.462\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.307999542284012\n",
      "Accuracy on hold-out set: 0.10235\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.305\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.154\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3101508236408232\n",
      "Accuracy on hold-out set: 0.09795\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.309\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3063142124652862\n",
      "Accuracy on hold-out set: 0.0986\n",
      "Epoch: 9\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.306\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3066399324417115\n",
      "Accuracy on hold-out set: 0.10005\n",
      "Epoch: 10\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.308\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 1.153\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.769\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.577\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.461\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.330\n",
      "Loss on hold-out set: 2.3051582182884216\n",
      "Accuracy on hold-out set: 0.10235\n",
      "Early stopping at epoch 9\n",
      "Returned to Spot: Validation loss: 2.3051582182884216\n",
      "----------------------------------------------\n",
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 2.124\n",
      "Loss on hold-out set: 1.749822355747223\n",
      "Accuracy on hold-out set: 0.3615\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.695\n",
      "Loss on hold-out set: 1.578557369661331\n",
      "Accuracy on hold-out set: 0.4326\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.542\n",
      "Loss on hold-out set: 1.4840718392372132\n",
      "Accuracy on hold-out set: 0.4677\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.466\n",
      "Loss on hold-out set: 1.39844347448349\n",
      "Accuracy on hold-out set: 0.4981\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.390\n",
      "Loss on hold-out set: 1.3632787815093994\n",
      "Accuracy on hold-out set: 0.5081\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.332\n",
      "Loss on hold-out set: 1.3153304802894592\n",
      "Accuracy on hold-out set: 0.52735\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.265\n",
      "Loss on hold-out set: 1.2859651947259902\n",
      "Accuracy on hold-out set: 0.54575\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.232\n",
      "Loss on hold-out set: 1.253812584066391\n",
      "Accuracy on hold-out set: 0.55375\n",
      "Epoch: 9\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.191\n",
      "Loss on hold-out set: 1.2553521171092987\n",
      "Accuracy on hold-out set: 0.55745\n",
      "Epoch: 10\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.160\n",
      "Loss on hold-out set: 1.2477548687696456\n",
      "Accuracy on hold-out set: 0.56645\n",
      "Epoch: 11\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.139\n",
      "Loss on hold-out set: 1.1819396452426911\n",
      "Accuracy on hold-out set: 0.58485\n",
      "Epoch: 12\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.091\n",
      "Loss on hold-out set: 1.187083414387703\n",
      "Accuracy on hold-out set: 0.5842\n",
      "Epoch: 13\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.067\n",
      "Loss on hold-out set: 1.2214792362451554\n",
      "Accuracy on hold-out set: 0.57555\n",
      "Epoch: 14\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.052\n",
      "Loss on hold-out set: 1.1559967747449875\n",
      "Accuracy on hold-out set: 0.59395\n",
      "Epoch: 15\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.039\n",
      "Loss on hold-out set: 1.1485581492185593\n",
      "Accuracy on hold-out set: 0.5992\n",
      "Epoch: 16\n",
      "Batch:  1000. Batch Size: 16. Training Loss (running): 1.016\n",
      "Loss on hold-out set: 1.1302049226284028\n",
      "Accuracy on hold-out set: 0.6055\n",
      "Returned to Spot: Validation loss: 1.1302049226284028\n",
      "----------------------------------------------\n",
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.168\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.960\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.600\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.422\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.331\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.272\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.222\n",
      "Loss on hold-out set: 1.5365323803544044\n",
      "Accuracy on hold-out set: 0.43265\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.484\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.752\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.493\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.370\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.289\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.239\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.200\n",
      "Loss on hold-out set: 1.377549140137434\n",
      "Accuracy on hold-out set: 0.50135\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.364\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.666\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.448\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.332\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.260\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.217\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.187\n",
      "Loss on hold-out set: 1.3101284647598863\n",
      "Accuracy on hold-out set: 0.5344\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.226\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.615\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.419\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.300\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.243\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.201\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.173\n",
      "Loss on hold-out set: 1.2648765187442303\n",
      "Accuracy on hold-out set: 0.559\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.147\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.573\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.369\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.285\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.223\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.184\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.163\n",
      "Loss on hold-out set: 1.1824811413912102\n",
      "Accuracy on hold-out set: 0.59425\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.019\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.544\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.339\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.272\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.213\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.173\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.147\n",
      "Loss on hold-out set: 1.2484361719408072\n",
      "Accuracy on hold-out set: 0.5918\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.955\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.476\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.335\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.250\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.199\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.164\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.142\n",
      "Loss on hold-out set: 1.1627043986867183\n",
      "Accuracy on hold-out set: 0.61225\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.898\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.452\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.302\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.227\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.183\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.155\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.138\n",
      "Loss on hold-out set: 1.2012225326092216\n",
      "Accuracy on hold-out set: 0.61025\n",
      "Returned to Spot: Validation loss: 1.2012225326092216\n",
      "----------------------------------------------\n",
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 2.202\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.989\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.614\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.442\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.342\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.278\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.235\n",
      "Loss on hold-out set: 1.6120058484375477\n",
      "Accuracy on hold-out set: 0.4072\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.582\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.770\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.513\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.383\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.302\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.247\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.215\n",
      "Loss on hold-out set: 1.4745578678041698\n",
      "Accuracy on hold-out set: 0.4724\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.437\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.707\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.478\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.352\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.285\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.231\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.198\n",
      "Loss on hold-out set: 1.4063539038836956\n",
      "Accuracy on hold-out set: 0.49395\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.342\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.666\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.442\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.336\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.262\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.218\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.189\n",
      "Loss on hold-out set: 1.3541723544180393\n",
      "Accuracy on hold-out set: 0.5186\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.273\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.629\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.424\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.324\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.255\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.208\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.177\n",
      "Loss on hold-out set: 1.3065297397360205\n",
      "Accuracy on hold-out set: 0.5495\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.241\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.593\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.406\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.301\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.243\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.203\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.172\n",
      "Loss on hold-out set: 1.3287409924417735\n",
      "Accuracy on hold-out set: 0.5381\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.188\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.598\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.384\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.294\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.234\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.189\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.169\n",
      "Loss on hold-out set: 1.240744680877775\n",
      "Accuracy on hold-out set: 0.57095\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.122\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.580\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.385\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.284\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.227\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.194\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.165\n",
      "Loss on hold-out set: 1.2501474935628474\n",
      "Accuracy on hold-out set: 0.5732\n",
      "Epoch: 9\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.070\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.558\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.373\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.281\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.224\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.182\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.165\n",
      "Loss on hold-out set: 1.2373750589333474\n",
      "Accuracy on hold-out set: 0.57245\n",
      "Epoch: 10\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.081\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.545\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.371\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.274\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.212\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.182\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.157\n",
      "Loss on hold-out set: 1.1767046087048947\n",
      "Accuracy on hold-out set: 0.5952\n",
      "Epoch: 11\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.043\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.542\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.350\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.270\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.216\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.180\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.149\n",
      "Loss on hold-out set: 1.2499113192718476\n",
      "Accuracy on hold-out set: 0.58405\n",
      "Epoch: 12\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 1.046\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.514\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.350\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.268\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.209\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.173\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.149\n",
      "Loss on hold-out set: 1.2149099456652999\n",
      "Accuracy on hold-out set: 0.5898\n",
      "Epoch: 13\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.999\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.518\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.332\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.258\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.207\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.177\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.146\n",
      "Loss on hold-out set: 1.1726018712453543\n",
      "Accuracy on hold-out set: 0.60385\n",
      "Epoch: 14\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.981\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.496\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.346\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.254\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.203\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.168\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.143\n",
      "Loss on hold-out set: 1.1789839990332722\n",
      "Accuracy on hold-out set: 0.6029\n",
      "Epoch: 15\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.988\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.498\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.321\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.250\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.201\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.166\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.142\n",
      "Loss on hold-out set: 1.189289076116169\n",
      "Accuracy on hold-out set: 0.6063\n",
      "Epoch: 16\n",
      "Batch:  1000. Batch Size: 4. Training Loss (running): 0.959\n",
      "Batch:  2000. Batch Size: 4. Training Loss (running): 0.471\n",
      "Batch:  3000. Batch Size: 4. Training Loss (running): 0.325\n",
      "Batch:  4000. Batch Size: 4. Training Loss (running): 0.242\n",
      "Batch:  5000. Batch Size: 4. Training Loss (running): 0.196\n",
      "Batch:  6000. Batch Size: 4. Training Loss (running): 0.169\n",
      "Batch:  7000. Batch Size: 4. Training Loss (running): 0.143\n",
      "Loss on hold-out set: 1.212369004075695\n",
      "Accuracy on hold-out set: 0.6009\n",
      "Returned to Spot: Validation loss: 1.212369004075695\n",
      "----------------------------------------------\n",
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 2.071\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.894\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.553\n",
      "Loss on hold-out set: 1.6055228410005569\n",
      "Accuracy on hold-out set: 0.4086\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.570\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.759\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.491\n",
      "Loss on hold-out set: 1.4646262445807456\n",
      "Accuracy on hold-out set: 0.46865\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.405\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.681\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.454\n",
      "Loss on hold-out set: 1.366226025724411\n",
      "Accuracy on hold-out set: 0.5074\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.287\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.630\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.412\n",
      "Loss on hold-out set: 1.3018863455057144\n",
      "Accuracy on hold-out set: 0.5373\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.187\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.593\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.385\n",
      "Loss on hold-out set: 1.2503702186584473\n",
      "Accuracy on hold-out set: 0.56175\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.085\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.544\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.372\n",
      "Loss on hold-out set: 1.1772640739858151\n",
      "Accuracy on hold-out set: 0.58525\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.006\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.520\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.349\n",
      "Loss on hold-out set: 1.1740171286821366\n",
      "Accuracy on hold-out set: 0.593\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.969\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.491\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.319\n",
      "Loss on hold-out set: 1.156074733416736\n",
      "Accuracy on hold-out set: 0.6088\n",
      "Epoch: 9\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.882\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.452\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.312\n",
      "Loss on hold-out set: 1.1923739310950041\n",
      "Accuracy on hold-out set: 0.6029\n",
      "Epoch: 10\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.834\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.422\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.294\n",
      "Loss on hold-out set: 1.2203877257883549\n",
      "Accuracy on hold-out set: 0.59395\n",
      "Epoch: 11\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.769\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.396\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.271\n",
      "Loss on hold-out set: 1.1611144548505545\n",
      "Accuracy on hold-out set: 0.62445\n",
      "Epoch: 12\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.724\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.371\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.256\n",
      "Loss on hold-out set: 1.2011382393628358\n",
      "Accuracy on hold-out set: 0.6161\n",
      "Epoch: 13\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 0.644\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.337\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.243\n",
      "Loss on hold-out set: 1.219091125024855\n",
      "Accuracy on hold-out set: 0.6193\n",
      "Early stopping at epoch 12\n",
      "Returned to Spot: Validation loss: 1.219091125024855\n",
      "----------------------------------------------\n",
      "Epoch: 1\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 2.065\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.907\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.569\n",
      "Loss on hold-out set: 1.6406577454805373\n",
      "Accuracy on hold-out set: 0.37865\n",
      "Epoch: 2\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.603\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.778\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.506\n",
      "Loss on hold-out set: 1.44305860465765\n",
      "Accuracy on hold-out set: 0.4793\n",
      "Epoch: 3\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.444\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.714\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.466\n",
      "Loss on hold-out set: 1.3825899273395539\n",
      "Accuracy on hold-out set: 0.5034\n",
      "Epoch: 4\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.356\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.676\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.449\n",
      "Loss on hold-out set: 1.392081136161089\n",
      "Accuracy on hold-out set: 0.51265\n",
      "Epoch: 5\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.295\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.661\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.427\n",
      "Loss on hold-out set: 1.3276797334909438\n",
      "Accuracy on hold-out set: 0.5267\n",
      "Epoch: 6\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.269\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.630\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.423\n",
      "Loss on hold-out set: 1.325842610025406\n",
      "Accuracy on hold-out set: 0.53455\n",
      "Epoch: 7\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.223\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.619\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.405\n",
      "Loss on hold-out set: 1.3418296605944633\n",
      "Accuracy on hold-out set: 0.5212\n",
      "Epoch: 8\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.199\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.596\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.407\n",
      "Loss on hold-out set: 1.24870584679842\n",
      "Accuracy on hold-out set: 0.5588\n",
      "Epoch: 9\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.156\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.597\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.393\n",
      "Loss on hold-out set: 1.2951732468008994\n",
      "Accuracy on hold-out set: 0.54845\n",
      "Epoch: 10\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.122\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.575\n",
      "Batch:  3000. Batch Size: 8. Training Loss (running): 0.386\n",
      "Loss on hold-out set: 1.2838199335813523\n",
      "Accuracy on hold-out set: 0.5553\n",
      "Epoch: 11\n",
      "Batch:  1000. Batch Size: 8. Training Loss (running): 1.127\n",
      "Batch:  2000. Batch Size: 8. Training Loss (running): 0.579\n"
     ]
    }
   ],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = MAX_TIME,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      \"log_level\": 50\n",
    "                                      })\n",
    "spot_tuner.run(X_start=X_start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "LOAD = False\n",
    "\n",
    "if SAVE:\n",
    "    result_file_name = \"res_\" + experiment_name + \".pkl\"\n",
    "    with open(result_file_name, 'wb') as f:\n",
    "        pickle.dump(spot_tuner, f)\n",
    "\n",
    "if LOAD:\n",
    "    result_file_name = \"res_ch10-friedman-hpt-0_maans03_60min_20init_1K_2023-04-14_10-11-19.pkl\"\n",
    "    with open(result_file_name, 'rb') as f:\n",
    "        spot_tuner =  pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the Progress of the hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=\"../Figures.d/\" + experiment_name+\"_progress.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=0.025, filename=\"../Figures.d/\" + experiment_name+\"_importance.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_default = get_default_values(fun_control)\n",
    "values_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\n",
    "values_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default = fun_control[\"core_model\"](**values_default)\n",
    "model_default"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SPOT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "return_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spot = get_one_core_model_from_X(X, fun_control)\n",
    "model_spot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = fun_control[\"test\"]\n",
    "testset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.evaluate_hold_out(dataset = testset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spot.evaluate_hold_out(dataset = testset, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Train Data and Test on Test Data (Hold-Out)\n",
    "\n",
    "Final Evaluation as in the Pytorch tutorial [Hyperparameter Tuning  with Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = fun_control[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.evaluate_hold_out(dataset=trainset, shuffle=False, test_dataset=testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spot.evaluate_hold_out(dataset=trainset, shuffle=False, test_dataset=testset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Hyperparameter Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For productive use, you might want to select:\n",
    "  * `min_z=min(spot_tuner.y)` and\n",
    "  * `max_z = max(spot_tuner.y)`\n",
    "* These settings are not so colorful as visualizations that use `None` for the ranges, but give better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.025\n",
    "impo = spot_tuner.print_importance(threshold=threshold, print_screen=True)\n",
    "var_plots = [i for i, x in enumerate(impo) if x[1] > threshold]\n",
    "min_z = min(spot_tuner.y)\n",
    "max_z = max(spot_tuner.y)\n",
    "n = spot_tuner.k\n",
    "for i in var_plots:\n",
    "    for j in var_plots:\n",
    "        if j > i:\n",
    "            filename = \"../Figures.d/\" + experiment_name+\"_contour_\"+str(i)+\"_\"+str(j)+\".pdf\"\n",
    "            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Coordinates Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all Combinations of Hyperparameters\n",
    "\n",
    "* Warning: this may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_ALL = False\n",
    "if PLOT_ALL:\n",
    "    n = spot_tuner.k\n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1, n):\n",
    "            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
