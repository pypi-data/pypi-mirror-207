"""
T-Digest serves as a statistics tool. It can provide
more precise result of quantile than usual tools.
The whole computation can be computed in a reasonable
time.
"""

import numpy as np

from ..base import KVNode, SLMixin
from ..metrics.pairwise._kl_div import kl_div
from ..utils._check import (
    check_valid_int,
    check_valid_float,
    check_pairwise_1d_array
)


def _scale_function(q, delta=57):
    r"""The size bound of t-digests is imposed using a scale
    function that forces clusters near the beginning or end
    of the digest to be small, possibly containing only a single
    sample. The scale function is chosen to provide an appropriate
    trade-off between very accurate quantile estimate in the tails
    of a distribution, reasonable accuracy near the median while
    keeping the number of clusters as small as possible.

    To limit cluster size in this way, we define the scale function
    as a non-decreasing function from quantile `q` to a notional index
    `k` with compression parameter :math:`\delta`. A one commonly used
    scale function for the t-digest is

        .. math::
            k_1(q) = \frac{\delta}{2 \pi}\sin^{-1}(2q - 1)

    Parameters
    ----------
    q : float
        The quantile to be translated.
    delta : int, optional
        The :math:`\delta` parameter in scalar function. The bigger
        :math:`\delta` is, the more clusters in t-digest.

    Returns
    -------
    float
        The translated notional index.
    """

    return (delta / (2 * np.pi)) * np.arcsin(2 * q - 1)


class TDigest(SLMixin):
    r"""A t-digest is generated by clustering real-valued samples and
    retaining the mean and number of samples for each cluster. This
    clustering can then be used to estimate quantile-related statistics
    with particularly high accuracy near the tails of a distribution.

    Algorithmically, there are two important ways to form a t-digest from
    a set of numbers. One version keeps a buffer of incoming samples. When
    the buffer fills, the contents are sorted and merged with the centroids
    computed from previous samples.

    This merging form of the t-digest algorithm has the virtue of allowing
    all memory structures to be allocated statically. On an amortized basis,
    this buffer-and-merge algorithm can be very fast especially if the input
    buffer is large. The other major t-digest algorithm is more akin to
    traditional clustering algorithms where new samples are added one at a
    time to whichever cluster is nearest.

    Parameters
    ----------
    buffer_size : int
        The size of buffer used in t-digest.
    scale_function : pyfunc, optional
        The python function to translate a quantile to a notional index.
        When scale_function is None, the default scalar function will be used.
    scale_function_params : dict, optional
        The parameter of scalar function to be used in scale_function.

    Attributes
    ----------
    buffer_size_ : int
        The size of buffer used in t-digest.
    scale_function_ : pyfunc, optional
        The python function to translate a quantile to a notional index.
        When scale_function is None, the default scalar function will be used.
    scale_function_params_ : dict, optional
        The parameter of scalar function to be used in scale_function.
    X_ : array_like, shape (n_samples)
        The input array.
    buffer_ : list
        The buffer to temporarily store the values added.
    clusters_ : list
        The clusters computed from previously added values.
    min_ : float
        The minimal value in the input array X.
    max_ : float
        The maximal value in the input array X.
    total_num_ : int
        The total number of values in the input array X.
    quantiles_ : array_like
        The list of precomputed highlighted quantiles.
    values_ : array_like
        The list of precomputed highlighted values corresponding
        to the quantiles.
    """

    def __init__(self, buffer_size,
                 scale_function=None,
                 scale_function_params=None):

        self.buffer_size_ = check_valid_int(
            buffer_size,
            lower=1,
            variable_name='buffer size'
        )

        if scale_function is None:
            scale_function = _scale_function

        if scale_function_params is None:
            self.scale_function_ = scale_function
        else:
            self.scale_function_ = scale_function(**scale_function_params)

        self.scale_function_params_ = scale_function_params

        # Data Storage
        self.buffer_ = []
        self.clusters_ = []

        # Statistics Information
        self.total_num_ = 0
        self.min_ = float('inf')
        self.max_ = float('-inf')

        return

    def _merge_clusters(self):
        r"""This function will merge precomputed clusters according to the
        preset scalar function.

        Given this complete ordering, we define a left and right weight
        for each cluster :math:`C_i` as the sum of the weights
        of clusters to the left and right of :math:`C_i`. That
        is

            .. math::
                W_{left}(C_i) = \sum_{j \lt i} |C_j| \\
                W_{right}(C_i) = \sum_{j \gt i} |C_j|

        The scale function provides the necessary mechanism to define
        the size bound of a t-digest. Every cluster :math:`C` with more
        than one sample should have the :math:`k`-size (written as
        :math:`|C|_k`) at most 1,

            .. math::
                |C|_k = k(q_{right}) - k(q_{left}) \le 1

        where

            .. math::
                q_{left} = W_{left}(C) / n \\
                q_{right} = q_{left} + |C| / n

        In a fully merged t-digest adjacent clusters, cannot be merged
        because the result would be too big

            .. math::
                |C_i \cup C_{i+1}|_k = |C_i|_k + |C_{i+1}|_k \gt 1
        """

        if len(self.clusters_) < 2:
            return

        new_clusters_ = []
        first_cluster = self.clusters_[0]

        for i in range(1, len(self.clusters_)):
            second_cluster = self.clusters_[i]

            q_left = 0
            for node in new_clusters_:
                q_left += node.key_

            q_right = q_left + first_cluster.key_ + second_cluster.key_

            q_left /= self.total_num_
            q_right /= self.total_num_

            if self.scale_function_(q_right) - \
                    self.scale_function_(q_left) <= 1:

                new_node_key_ = first_cluster.key_ + second_cluster.key_
                new_node_value_ = \
                    first_cluster.key_ * first_cluster.value_ + \
                    second_cluster.key_ * second_cluster.value_
                new_node_value_ /= new_node_key_

                new_node = KVNode(new_node_key_, new_node_value_)

                first_cluster = new_node
            else:
                new_clusters_.append(first_cluster)

                first_cluster = second_cluster

            second_cluster = None

        if first_cluster:
            new_clusters_.append(first_cluster)
        if second_cluster:
            new_clusters_.append(second_cluster)

        self.clusters_ = new_clusters_

        return

    def _flush_buffer(self):
        r"""Flush the TDigest buffer

        This function will move the data in buffer to clusters, and clear
        the buffer afterwards. When the data is added to clusters, clusters
        will be sorted, and try to be merged using function `_merged_clusters`
        if possible.
        """

        if len(self.buffer_) == 0:
            return

        self.clusters_.extend(self.buffer_)
        self.buffer_ = []

        self.clusters_ = sorted(
            self.clusters_,
            key=lambda x: x.value_,
        )
        self._merge_clusters()

        return

    def _fit_one(self, value):
        r"""Fit one value into TDigest

        This function will feed one value into TDigest. Since this version
        of TDigest is fed with one value at a time to the buffer.

        Whenever a value is fed, the total number is added. If the size of
        buffer is larger than buffer_size, the function `_flush_buffer` will
        be triggered.

        For reference, a base data structure `KVNode` is used to store data in
        the following format `KVNode(cluster_cnt, cluster_value)`. Each new
        value is treated as `KVNode(1, value)`. And this data structure is also
        used to store the cluster inside clusters.

        Parameters
        ----------
        value : float
            A single value from the input array X.
        """

        node = KVNode(1, value)
        self.buffer_.append(node)
        self.total_num_ += 1

        if len(self.buffer_) >= self.buffer_size_:
            self._flush_buffer()

        return

    def fit(self, X, Y=None):
        r"""Fit a TDigest

        All the input data is provided by X, while Y is set to None
        to be ignored. In TDigest, this function copy the input X as
        the attribute and fit the TDigest.

        The attribute `smallest` and `largest` are calculated during
        the process. When the TDigest is fitted, attribute `quantiles`
        and `values` are calculated. The length of `quantiles` and `values`
        are of the same length. Both record the key quantiles and corresponding
        values.

        Parameters
        ----------
        X : array_like, shape (n_samples)
            The input array.
        Y : Ignored
            Not used, present for scikit-learn API consistency by convention.

        Returns
        -------
        self : object
            TDigest class object itself.
        """

        X, Y = check_pairwise_1d_array(X, Y)

        self.X_ = X

        self.min_ = np.amin(X)
        self.max_ = np.amax(X)

        _ = np.vectorize(self._fit_one, otypes=[object])(X)
        self._flush_buffer()

        counts, values = zip(
            *[
                [cluster.key_, cluster.value_]
                for cluster in self.clusters_
            ]
        )

        quantiles = np.insert(np.cumsum(counts), 0, 0)[:-1] + \
            np.array(counts)
        quantiles = quantiles / self.total_num_

        self.quantiles_ = np.concatenate([[0], quantiles])
        self.values_ = np.concatenate([[self.min_], values])

        return self

    def _predict_one(self, quantile):
        r"""Predict one quantile using TDigest

        This function will predict one target quantile using TDigest.

        If the quantile is 0 to be searched, the smallest value will
        be returned.

        If the quantile is 1 to be searched, the largest value will
        be returned.

        In the other cases, it will try to search the quantile with its
        adjacent two clusters. The slope will be calculated between the
        clusters. The slope will be used as a linear interpolation to
        calculate the estimated value.

        Parameters
        ----------
        quantile : float
            The quantile to estimate.

        Returns
        -------
        estimate : float
            The estimate value to corresponding quantile.
        """

        quantile = check_valid_float(
            quantile,
            lower=0.0, upper=1.0,
            variable_name='quantile'
        )

        if quantile == 0:
            return self.min_
        if quantile == 1:
            return self.max_

        index = np.searchsorted(self.quantiles_, quantile, side='right')
        slopes = (self.values_[index] - self.values_[index - 1]) / \
            (self.quantiles_[index] - self.quantiles_[index - 1])

        estimate = self.values_[index - 1] + \
            slopes * (quantile - self.quantiles_[index - 1])

        return estimate

    def predict(self, X, Y=None):
        r"""Predict a list of quantiles using TDigest

        This function will predict each quantile with function
        `_predict_one` to obtain the corresponding estimated values.

        Parameters
        ----------
        X : array_like, shape (n_samples)
            The input array.
        Y : Ignored
            Not used, present for scikit-learn API consistency by convention.

        Returns
        -------
        estimates : array_like, shape (n_samples)
            The estimated values to the quantiles.
        """

        X, Y = check_pairwise_1d_array(X, Y)

        estimates = np.vectorize(self._predict_one, otypes=[np.float_])(X)

        return estimates

    @staticmethod
    def t_digest_divergence(t_digest1, t_digest2):
        r"""Calculate the divergence between two TDigests

        This function will calculate the divergence between two TDigests.
        It will retrieve the counts and values from every TDigest, and then
        calculate the KL divergence between them.

        Parameters
        ----------
        t_digest1 : object
            The first TDigest class object.
        t_digest2 : object
            The second TDigest class object.

        Returns
        -------
        divergence : float
            The divergence between two TDigests.
        """

        counts1, values1 = zip(
            *[
                [cluster.key_, cluster.value_]
                for cluster in t_digest1.clusters_
            ]
        )

        counts2, values2 = zip(
            *[
                [cluster.key_, cluster.value_]
                for cluster in t_digest2.clusters_
            ]
        )

        divergence = kl_div(
            values1, values2,
            weights_X=counts1, weights_Y=counts2,
        )

        return divergence

    def divergence(self, other):
        r"""Calculate the divergence to the other TDigest

        This function will use the class statistic method
        `t_digest_divergence` to calculate the divergence
        between self and the other.


        Parameters
        ----------
        other : object
            The other TDigest class object.

        Returns
        -------
        float
            The divergence between two TDigests.
        """

        return TDigest.t_digest_divergence(self, other)
